{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "janus": {
     "all_versions_showing": false,
     "cell_hidden": false,
     "current_version": 0,
     "id": "0457f59f6bd62",
     "named_versions": [],
     "output_hidden": false,
     "show_versions": false,
     "source_hidden": false,
     "versions": []
    }
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"shakespeare.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    # preprocess data for the first time.\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "\n",
    "    # load the preprocessed the data if the data has been processed before.\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "    # seperate the whole data into different batches.\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small,\n",
    "        # let's give them a better error message\n",
    "        if self.num_batches == 0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        # reshape the original data into the length self.num_batches * self.batch_size * self.seq_length for convenience.\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "\n",
    "        #ydata is the xdata with one position shift.\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1),\n",
    "                                  self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1),\n",
    "                                  self.num_batches, 1)\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "janus": {
     "all_versions_showing": false,
     "cell_hidden": false,
     "current_version": 0,
     "id": "f4cb1ce540ac1",
     "named_versions": [],
     "output_hidden": false,
     "show_versions": false,
     "source_hidden": false,
     "versions": []
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR] [--save_dir SAVE_DIR]\n",
      "                             [--log_dir LOG_DIR] [--save_every SAVE_EVERY]\n",
      "                             [--init_from INIT_FROM] [--model MODEL]\n",
      "                             [--rnn_size RNN_SIZE] [--num_layers NUM_LAYERS]\n",
      "                             [--seq_length SEQ_LENGTH]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--num_epochs NUM_EPOCHS] [--grad_clip GRAD_CLIP]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--decay_rate DECAY_RATE]\n",
      "                             [--output_keep_prob OUTPUT_KEEP_PROB]\n",
      "                             [--input_keep_prob INPUT_KEEP_PROB]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/AdityaS/Library/Jupyter/runtime/kernel-d843b758-f43f-45c4-9ee2-e6b703fe6a92.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Data and model checkpoints directories\n",
    "parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare',\n",
    "                    help='data directory containing input.txt with training examples')\n",
    "parser.add_argument('--save_dir', type=str, default='save',\n",
    "                    help='directory to store checkpointed models')\n",
    "parser.add_argument('--log_dir', type=str, default='logs',\n",
    "                    help='directory to store tensorboard logs')\n",
    "parser.add_argument('--save_every', type=int, default=1000,\n",
    "                    help='Save frequency. Number of passes between checkpoints of the model.')\n",
    "parser.add_argument('--init_from', type=str, default=None,\n",
    "                    help=\"\"\"continue training from saved model at this path (usually \"save\").\n",
    "                        Path must contain files saved by previous training process:\n",
    "                        'config.pkl'        : configuration;\n",
    "                        'chars_vocab.pkl'   : vocabulary definitions;\n",
    "                        'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                              Note: this file contains absolute paths, be careful when moving files around;\n",
    "                        'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                         Model params must be the same between multiple runs (model, rnn_size, num_layers and seq_length).\n",
    "                    \"\"\")\n",
    "# Model params\n",
    "parser.add_argument('--model', type=str, default='lstm',\n",
    "                    help='lstm, rnn, gru, or nas')\n",
    "parser.add_argument('--rnn_size', type=int, default=128,\n",
    "                    help='size of RNN hidden state')\n",
    "parser.add_argument('--num_layers', type=int, default=2,\n",
    "                    help='number of layers in the RNN')\n",
    "# Optimization\n",
    "parser.add_argument('--seq_length', type=int, default=50,\n",
    "                    help='RNN sequence length. Number of timesteps to unroll for.')\n",
    "parser.add_argument('--batch_size', type=int, default=50,\n",
    "                    help=\"\"\"minibatch size. Number of sequences propagated through the network in parallel.\n",
    "                            Pick batch-sizes to fully leverage the GPU (e.g. until the memory is filled up)\n",
    "                            commonly in the range 10-500.\"\"\")\n",
    "parser.add_argument('--num_epochs', type=int, default=50,\n",
    "                    help='number of epochs. Number of full passes through the training examples.')\n",
    "parser.add_argument('--grad_clip', type=float, default=5.,\n",
    "                    help='clip gradients at this value')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.002,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--decay_rate', type=float, default=0.97,\n",
    "                    help='decay rate for rmsprop')\n",
    "parser.add_argument('--output_keep_prob', type=float, default=1.0,\n",
    "                    help='probability of keeping weights in the hidden layer')\n",
    "parser.add_argument('--input_keep_prob', type=float, default=1.0,\n",
    "                    help='probability of keeping weights in the input layer')\n",
    "args = parser.parse_args()\n",
    "\n",
    "import tensorflow as tf\n",
    "from utils import TextLoader\n",
    "from model import Model\n",
    "\n",
    "def train(args):\n",
    "    data_loader = TextLoader(args.data_dir, args.batch_size, args.seq_length)\n",
    "    args.vocab_size = data_loader.vocab_size\n",
    "\n",
    "    # check compatibility if training is continued from previously saved model\n",
    "    if args.init_from is not None:\n",
    "        # check if all necessary files exist\n",
    "        assert os.path.isdir(args.init_from),\" %s must be a a path\" % args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"config.pkl\")),\"config.pkl file does not exist in path %s\"%args.init_from\n",
    "        assert os.path.isfile(os.path.join(args.init_from,\"chars_vocab.pkl\")),\"chars_vocab.pkl.pkl file does not exist in path %s\" % args.init_from\n",
    "        ckpt = tf.train.latest_checkpoint(args.init_from)\n",
    "        assert ckpt, \"No checkpoint found\"\n",
    "\n",
    "        # open old config and check if models are compatible\n",
    "        with open(os.path.join(args.init_from, 'config.pkl'), 'rb') as f:\n",
    "            saved_model_args = cPickle.load(f)\n",
    "        need_be_same = [\"model\", \"rnn_size\", \"num_layers\", \"seq_length\"]\n",
    "        for checkme in need_be_same:\n",
    "            assert vars(saved_model_args)[checkme]==vars(args)[checkme],\"Command line argument and saved model disagree on '%s' \"%checkme\n",
    "\n",
    "        # open saved vocab/dict and check if vocabs/dicts are compatible\n",
    "        with open(os.path.join(args.init_from, 'chars_vocab.pkl'), 'rb') as f:\n",
    "            saved_chars, saved_vocab = cPickle.load(f)\n",
    "        assert saved_chars==data_loader.chars, \"Data and loaded model disagree on character set!\"\n",
    "        assert saved_vocab==data_loader.vocab, \"Data and loaded model disagree on dictionary mappings!\"\n",
    "\n",
    "    if not os.path.isdir(args.save_dir):\n",
    "        os.makedirs(args.save_dir)\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'wb') as f:\n",
    "        cPickle.dump(args, f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'wb') as f:\n",
    "        cPickle.dump((data_loader.chars, data_loader.vocab), f)\n",
    "\n",
    "    model = Model(args)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # instrument for tensorboard\n",
    "        summaries = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\n",
    "                os.path.join(args.log_dir, time.strftime(\"%Y-%m-%d-%H-%M-%S\")))\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        # restore model\n",
    "        if args.init_from is not None:\n",
    "            saver.restore(sess, ckpt)\n",
    "        for e in range(args.num_epochs):\n",
    "            sess.run(tf.assign(model.lr,\n",
    "                               args.learning_rate * (args.decay_rate ** e)))\n",
    "            data_loader.reset_batch_pointer()\n",
    "            state = sess.run(model.initial_state)\n",
    "            for b in range(data_loader.num_batches):\n",
    "                start = time.time()\n",
    "                x, y = data_loader.next_batch()\n",
    "                feed = {model.input_data: x, model.targets: y}\n",
    "                for i, (c, h) in enumerate(model.initial_state):\n",
    "                    feed[c] = state[i].c\n",
    "                    feed[h] = state[i].h\n",
    "\n",
    "                # instrument for tensorboard\n",
    "                summ, train_loss, state, _ = sess.run([summaries, model.cost, model.final_state, model.train_op], feed)\n",
    "                writer.add_summary(summ, e * data_loader.num_batches + b)\n",
    "\n",
    "                end = time.time()\n",
    "                print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                      .format(e * data_loader.num_batches + b,\n",
    "                              args.num_epochs * data_loader.num_batches,\n",
    "                              e, train_loss, end - start))\n",
    "                if (e * data_loader.num_batches + b) % args.save_every == 0\\\n",
    "                        or (e == args.num_epochs-1 and\n",
    "                            b == data_loader.num_batches-1):\n",
    "                    # save for the last result\n",
    "                    checkpoint_path = os.path.join(args.save_dir, 'model.ckpt')\n",
    "                    saver.save(sess, checkpoint_path,\n",
    "                               global_step=e * data_loader.num_batches + b)\n",
    "                    print(\"model saved to {}\".format(checkpoint_path))\n",
    "\n",
    "\n",
    "train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "janus": {
     "all_versions_showing": false,
     "cell_hidden": false,
     "current_version": 0,
     "id": "a654989347fa5",
     "named_versions": [],
     "output_hidden": false,
     "show_versions": false,
     "source_hidden": false,
     "versions": []
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--save_dir SAVE_DIR] [-n N] [--prime PRIME]\n",
      "                             [--sample SAMPLE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/AdityaS/Library/Jupyter/runtime/kernel-d843b758-f43f-45c4-9ee2-e6b703fe6a92.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "\n",
    "\n",
    "from six import text_type\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "                   formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--save_dir', type=str, default='save',\n",
    "                    help='model directory to store checkpointed models')\n",
    "parser.add_argument('-n', type=int, default=500,\n",
    "                    help='number of characters to sample')\n",
    "parser.add_argument('--prime', type=text_type, default=u'',\n",
    "                    help='prime text')\n",
    "parser.add_argument('--sample', type=int, default=1,\n",
    "                    help='0 to use max at each timestep, 1 to sample at '\n",
    "                         'each timestep, 2 to sample on spaces')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "import tensorflow as tf\n",
    "from model import Model\n",
    "\n",
    "def sample(args):\n",
    "    with open(os.path.join(args.save_dir, 'config.pkl'), 'rb') as f:\n",
    "        saved_args = cPickle.load(f)\n",
    "    with open(os.path.join(args.save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    #Use most frequent char if no prime is given\n",
    "    if args.prime == '':\n",
    "        args.prime = chars[0]\n",
    "    model = Model(saved_args, training=False)\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "        ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            data = model.sample(sess, chars, vocab, args.n, args.prime,\n",
    "                               args.sample).encode('utf-8')\n",
    "            print(data.decode(\"utf-8\"))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sample(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "janus": {
     "all_versions_showing": false,
     "cell_hidden": false,
     "current_version": 0,
     "id": "49d4a653ab506",
     "named_versions": [],
     "output_hidden": false,
     "show_versions": false,
     "source_hidden": false,
     "versions": []
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, args, training=True):\n",
    "        self.args = args\n",
    "        if not training:\n",
    "            args.batch_size = 1\n",
    "            args.seq_length = 1\n",
    "\n",
    "        # choose different rnn cell \n",
    "        if args.model == 'rnn':\n",
    "            cell_fn = rnn.RNNCell\n",
    "        elif args.model == 'gru':\n",
    "            cell_fn = rnn.GRUCell\n",
    "        elif args.model == 'lstm':\n",
    "            cell_fn = rnn.LSTMCell\n",
    "        elif args.model == 'nas':\n",
    "            cell_fn = rnn.NASCell\n",
    "        else:\n",
    "            raise Exception(\"model type not supported: {}\".format(args.model))\n",
    "\n",
    "        # warp multi layered rnn cell into one cell with dropout\n",
    "        cells = []\n",
    "        for _ in range(args.num_layers):\n",
    "            cell = cell_fn(args.rnn_size)\n",
    "            if training and (args.output_keep_prob < 1.0 or args.input_keep_prob < 1.0):\n",
    "                cell = rnn.DropoutWrapper(cell,\n",
    "                                          input_keep_prob=args.input_keep_prob,\n",
    "                                          output_keep_prob=args.output_keep_prob)\n",
    "            cells.append(cell)\n",
    "        self.cell = cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # input/target data (int32 since input is char-level)\n",
    "        self.input_data = tf.placeholder(\n",
    "            tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.targets = tf.placeholder(\n",
    "            tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        # softmax output layer, use softmax to classify\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\",\n",
    "                                        [args.rnn_size, args.vocab_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
    "\n",
    "        # transform input to embedding\n",
    "        embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        # dropout beta testing: double check which one should affect next line\n",
    "        if training and args.output_keep_prob:\n",
    "            inputs = tf.nn.dropout(inputs, args.output_keep_prob)\n",
    "\n",
    "        # unstack the input to fits in rnn model\n",
    "        inputs = tf.split(inputs, args.seq_length, 1)\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "        # loop function for rnn_decoder, which take the previous i-th cell's output and generate the (i+1)-th cell's input\n",
    "        def loop(prev, _):\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        # rnn_decoder to generate the ouputs and final state. When we are not training the model, we use the loop function.\n",
    "        outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if not training else None, scope='rnnlm')\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, args.rnn_size])\n",
    "\n",
    "        # output layer\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "        # loss is calculate by the log loss and taking the average.\n",
    "        loss = legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([args.batch_size * args.seq_length])])\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        # calculate gradients\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                args.grad_clip)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "\n",
    "        # apply gradient change to the all the trainable variable.\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # instrument tensorboard\n",
    "        tf.summary.histogram('logits', self.logits)\n",
    "        tf.summary.histogram('loss', loss)\n",
    "        tf.summary.scalar('train_loss', self.cost)\n",
    "\n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.cell.zero_state(1, tf.float32))\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for _ in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else:  # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "janus": {
     "all_versions_showing": false,
     "cell_hidden": false,
     "current_version": 0,
     "id": "23ca0b840bea5",
     "named_versions": [],
     "output_hidden": false,
     "show_versions": false,
     "source_hidden": false,
     "versions": []
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "janus": {
   "filepaths": [
    [
     "24c26d38",
     1560201920219,
     1560201920219
    ],
    [
     "38223bae",
     1560201959248,
     1560202462443
    ]
   ],
   "janus_markers": [],
   "track_history": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
